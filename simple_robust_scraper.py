#!/usr/bin/env python3
"""
Scraper robusto simplificado para sites oficiais de seguran√ßa p√∫blica
Foco em fontes oficiais e confi√°veis
"""

import requests
from bs4 import BeautifulSoup
import time
import logging
from datetime import datetime, timedelta
import re

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleRobustScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # Configura√ß√µes para ignorar problemas de SSL
        self.session.verify = False
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # Configura√ß√µes de timeout e retry
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            method_whitelist=["HEAD", "GET", "OPTIONS"],
            backoff_factor=1
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
    def get_scraping_configs(self):
        """
        Configura√ß√µes de scraping apenas para fontes oficiais de seguran√ßa
        """
        return [
            {
                'name': 'PRF Nacional',
                'url': 'https://www.gov.br/prf/pt-br/noticias/ultimas/',
                'selectors': {
                    'articles': 'h2',  # PRF usa h2 para not√≠cias
                    'title': 'h2 a',  # T√≠tulos s√£o links dentro de h2
                    'link': 'h2 a',  # Links das not√≠cias
                    'date': '.documentByLine, .summary-view-icon, .date'  # Datas das not√≠cias
                },
                'rate_limit': 2.0
            },
            {
                'name': 'PF Nacional',
                'url': 'https://www.gov.br/pf/pt-br/assuntos/noticias/ultimas-noticias/',
                'selectors': {
                    'articles': 'article, .item, .noticia, .materia',
                    'title': 'h3 a, h2 a, .titulo a, .materia-titulo a',
                    'link': 'h3 a, h2 a, .titulo a, .materia-titulo a',
                    'date': '.data, .date, time, .materia-data'
                },
                'rate_limit': 2.0
            },
            {
                'name': 'MPRS',
                'url': 'https://www.mprs.mp.br/noticias/',
                'selectors': {
                    'articles': 'h2, a[href*="/noticias/"]',  # h2 para t√≠tulos e links diretos para not√≠cias
                    'title': 'h2 a, a[href*="/noticias/"]',   # T√≠tulos s√£o links dentro de h2 ou links diretos
                    'link': 'h2 a, a[href*="/noticias/"]',    # Links das not√≠cias
                    'date': '.data, .date, time, .materia-data, .publicado'  # Datas das not√≠cias
                },
                'rate_limit': 2.0
            },
            {
                'name': 'Pol√≠cia Civil',
                'url': 'https://www.pc.rs.gov.br/noticias/',
                'selectors': {
                    'articles': 'h3, h4, .item, .noticia, article',  # M√∫ltiplos selectors para capturar
                    'title': 'h3 a, h4 a, .titulo a, a',  # T√≠tulos das not√≠cias
                    'link': 'h3 a, h4 a, .titulo a, a',  # Links das not√≠cias
                    'date': '.data, .date, time, .timestamp'  # Datas das not√≠cias
                },
                'rate_limit': 2.0
            },
            {
                'name': 'Brigada Militar',
                'url': 'https://www.brigadamilitar.rs.gov.br/noticias/',
                'selectors': {
                    'articles': 'h3',  # Not√≠cias est√£o nos h3
                    'title': 'h3 a',  # T√≠tulos s√£o links dentro de h3
                    'link': 'h3 a',  # Links das not√≠cias
                    'date': '.data, .date, time, .timestamp, .news-date'  # Datas das not√≠cias
                },
                'rate_limit': 3.0
            },
            {
                'name': 'PM SC',
                'url': 'https://www.pm.sc.gov.br/noticias/',
                'selectors': {
                    'articles': 'a[href*="/noticias/"]',  # Links diretos para not√≠cias
                    'title': 'a[href*="/noticias/"]',  # T√≠tulo √© o texto do link
                    'link': 'a[href*="/noticias/"]',  # Link direto
                    'date': '.data, .date, time, .timestamp'
                },
                'rate_limit': 2.0
            },
            {
                'name': 'PM PR',
                'url': 'https://www.pmpr.pr.gov.br/Noticias/',
                'selectors': {
                    'articles': 'article, .item, .noticia, .materia',
                    'title': 'h3 a, h2 a, .titulo a, .materia-titulo a',
                    'link': 'h3 a, h2 a, .titulo a, .materia-titulo a',
                    'date': '.data, .date, time, .materia-data'
                },
                'rate_limit': 2.0
            },
            {
                'name': 'DOF MS',
                'url': 'https://www.dof.ms.gov.br/noticias/',
                'selectors': {
                    'articles': '.card, .card-body, .post',
                    'title': 'h5.card-title a, h5.card-title, .card-title a',
                    'link': 'h5.card-title a, .card-title a, a',
                    'date': '.card-text small, .date, time, .timestamp'
                },
                'rate_limit': 2.0
            },
            {
                'name': 'PC SC',
                'url': 'https://pc.sc.gov.br/noticias/',
                'selectors': {
                    'articles': 'h3, article',
                    'title': 'h3 a, h3',
                    'link': 'h3 a, h3',
                    'date': '.data, .date, time, .timestamp'
                },
                'rate_limit': 2.0
            },
            {
                'name': 'PC PR',
                'url': 'https://www.policiacivil.pr.gov.br/noticias/',
                'selectors': {
                    'articles': 'article, .item, h3, h4',
                    'title': 'h3 a, h4 a, .titulo a',
                    'link': 'h3 a, h4 a, .titulo a',
                    'date': 'time, .date, .data, .timestamp'
                },
                'rate_limit': 2.0
            }
        ]

    def is_relevant_news(self, title, content=""):
        """
        Verifica se a not√≠cia √© relevante baseada no t√≠tulo e conte√∫do
        Foco em crimes, opera√ß√µes policiais, tr√°fico, etc.
        """
        if not title:
            return False
            
        # Converte para min√∫sculas para compara√ß√£o
        title_lower = title.lower()
        content_lower = content.lower()
        text_to_check = f"{title_lower} {content_lower}"
        
        # Palavras-chave espec√≠ficas e precisas
        target_keywords = [
            'drogas', 'armas', 'maconha', 'coca√≠na', 'ecstasy', 'skunk', 
            'apreens√£o', 'pris√£o', 'tr√°fico', 'fac√ß√£o', 'opera√ß√£o',
            'gaeco', 'lavagem de dinheiro', 'contas abertas', 'investiga√ß√£o criminal',
            'bunker', 'entorpecentes', 'desmantela', 'grupo criminoso', 'narc√≥ticos', 'subst√¢ncias il√≠citas'
        ]
        
        # Verifica se alguma palavra-chave est√° presente
        for keyword in target_keywords:
            if keyword in text_to_check:
                return True
                
        return False

    def extract_news_data(self, element, selectors, base_url):
        """
        Extrai dados da not√≠cia de um elemento HTML
        """
        try:
            # T√≠tulo
            title_elem = element.select_one(selectors['title'])
            if not title_elem:
                # Tenta pegar o texto do pr√≥prio elemento se n√£o encontrar t√≠tulo
                title = element.get_text(strip=True)
            else:
                title = title_elem.get_text(strip=True)
            
            if not title or len(title) < 10:
                return None
            
            # Link
            link_elem = element.select_one(selectors['link'])
            if link_elem and link_elem.get('href'):
                link = link_elem['href']
                if link.startswith('/'):
                    # Remove /noticias/ da base_url se o link j√° cont√©m
                    base_clean = base_url.rstrip('/')
                    if base_clean.endswith('/noticias') and link.startswith('/noticias/'):
                        link = base_clean.replace('/noticias', '') + link
                    else:
                        link = base_clean + link
                elif not link.startswith('http'):
                    link = base_url.rstrip('/') + '/' + link
            else:
                # Se n√£o encontrar link, usa o pr√≥prio elemento
                if element.get('href'):
                    link = element['href']
                    if link.startswith('/'):
                        # Remove /noticias/ da base_url se o link j√° cont√©m
                        base_clean = base_url.rstrip('/')
                        if base_clean.endswith('/noticias') and link.startswith('/noticias/'):
                            link = base_clean.replace('/noticias', '') + link
                        else:
                            link = base_clean + link
                    elif not link.startswith('http'):
                        link = base_url.rstrip('/') + '/' + link
                else:
                    link = base_url
            
            # Data (tenta extrair, mas n√£o √© obrigat√≥ria)
            date_elem = element.select_one(selectors['date'])
            date_str = ""
            if date_elem:
                date_str = date_elem.get_text(strip=True)
            
            return {
                'title': title,
                'link': link,
                'date': date_str,
                'source': base_url
            }
            
        except Exception as e:
            logger.error(f"Erro ao extrair dados: {e}")
            return None

    def scrape_site(self, config):
        """
        Faz scraping de um site espec√≠fico
        """
        news_list = []
        
        try:
            logger.info(f"üîÑ Fazendo scraping: {config['name']}")
            
            # Rate limiting
            time.sleep(config.get('rate_limit', 2.0))
            
            # Tenta fazer a requisi√ß√£o com retry autom√°tico
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = self.session.get(config['url'], timeout=30)
                    response.raise_for_status()
                    break
                except (requests.exceptions.SSLError, requests.exceptions.ConnectionError, 
                        requests.exceptions.ConnectionResetError, requests.exceptions.Timeout,
                        requests.exceptions.ChunkedEncodingError, requests.exceptions.ConnectTimeout,
                        requests.exceptions.ReadTimeout, requests.exceptions.HTTPError) as e:
                    if attempt < max_retries - 1:
                        wait_time = 2 ** attempt  # Backoff exponencial: 2s, 4s, 8s
                        logger.warning(f"Tentativa {attempt + 1} falhou para {config['name']}: {e}")
                        logger.info(f"‚è≥ Aguardando {wait_time}s antes da pr√≥xima tentativa...")
                        time.sleep(wait_time)
                        continue
                    else:
                        logger.error(f"‚ùå Falha final ao acessar {config['name']} ap√≥s {max_retries} tentativas: {e}")
                        return []
                except Exception as e:
                    logger.error(f"‚ùå Erro inesperado ao acessar {config['name']}: {e}")
                    return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Busca por artigos/not√≠cias
            articles = soup.select(config['selectors']['articles'])
            logger.info(f"üì∞ Encontrados {len(articles)} elementos")
            
            for article in articles[:20]:  # Limita a 20 not√≠cias por site
                news_data = self.extract_news_data(article, config['selectors'], config['url'])
                
                if news_data and self.is_relevant_news(news_data['title']):
                    # Determina categoria baseada na palavra-chave
                    title_lower = news_data['title'].lower()
                    
                    if any(word in title_lower for word in ['gaeco', 'lavagem', 'investiga√ß√£o']):
                        news_data['category'] = "investiga√ß√£o"
                    elif any(word in title_lower for word in ['drogas', 'maconha', 'coca√≠na', 'ecstasy', 'skunk', 'bunker', 'entorpecentes', 'narc√≥ticos']):
                        news_data['category'] = "drogas"
                    elif 'armas' in title_lower:
                        news_data['category'] = "armas"
                    elif any(word in title_lower for word in ['tr√°fico', 'fac√ß√£o', 'grupo criminoso']):
                        news_data['category'] = "tr√°fico"
                    elif any(word in title_lower for word in ['apreens√£o', 'pris√£o', 'opera√ß√£o', 'desmantela']):
                        news_data['category'] = "policial"
                    else:
                        news_data['category'] = "geral"
                    
                    news_list.append(news_data)
                    logger.info(f"‚úÖ Not√≠cia relevante: {news_data['title'][:50]}...")
            
            logger.info(f"üéØ Total de not√≠cias relevantes encontradas: {len(news_list)}")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao fazer scraping de {config['name']}: {e}")
        
        return news_list

    def scrape_all_sites(self):
        """
        Faz scraping de todos os sites configurados
        """
        all_news = []
        configs = self.get_scraping_configs()
        
        logger.info(f"üöÄ Iniciando scraping de {len(configs)} sites oficiais...")
        
        for config in configs:
            try:
                site_news = self.scrape_site(config)
                all_news.extend(site_news)
                
                # Rate limiting entre sites
                time.sleep(1.0)
                
            except Exception as e:
                logger.error(f"‚ùå Erro ao processar {config['name']}: {e}")
                continue
        
        # Remove duplicatas baseadas no t√≠tulo
        unique_news = []
        seen_titles = set()
        
        for news in all_news:
            title_key = news['title'].lower().strip()
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        logger.info(f"üìä Total final: {len(unique_news)} not√≠cias √∫nicas")
        return unique_news

def main():
    """
    Fun√ß√£o principal para teste
    """
    scraper = SimpleRobustScraper()
    news = scraper.scrape_all_sites()
    
    print(f"\nüéØ RESULTADO FINAL: {len(news)} not√≠cias relevantes")
    print("=" * 60)
    
    for i, item in enumerate(news[:10], 1):  # Mostra apenas as primeiras 10
        print(f"\n{i}. [{item['category'].upper()}] {item['title']}")
        print(f"   üîó {item['link']}")
        if item['date']:
            print(f"   üìÖ {item['date']}")

if __name__ == "__main__":
    main()
